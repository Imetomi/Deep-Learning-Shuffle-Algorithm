{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.keras import backend as K\n",
    "from loop import TrainingLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'\n",
    "train_data = np.load(os.path.join(data_path, 'twitter_train_vectors.npy'), allow_pickle=True)\n",
    "test_data = np.load(os.path.join(data_path, 'twitter_test_vectors.npy'), allow_pickle=True)\n",
    "train_labels = np.load(os.path.join(data_path, 'twitter_train_labels.npy'), allow_pickle=True)\n",
    "test_labels = np.load(os.path.join(data_path, 'twitter_test_labels.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_on_window(data, label, size):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    for i in tqdm_n(range(len(data))):\n",
    "        sample = data[i]\n",
    "        if len(sample) >= size:\n",
    "            new_sample = []\n",
    "            count = int(np.floor((len(sample) / size)))\n",
    "            rest = int(len(sample) / size)\n",
    "            for j in range(size-1):\n",
    "                new_sample.append(np.array(sample[j*count:(j+1)*count].mean(axis=0)))\n",
    "            if rest != 0:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count+rest].mean(axis=0)))\n",
    "            else:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count].mean(axis=0)))\n",
    "            new_data.append(np.array(new_sample))\n",
    "            new_labels.append(label[i])\n",
    "    return np.array(new_data), np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=119999.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e84df87b43cb42978414c3cbb270357f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=40000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e9470974f244e83bc4838c627016668"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = average_on_window(train_data, train_labels, 2)\n",
    "X_test, y_test = average_on_window(test_data, test_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(39725, 2, 250)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(5, return_sequences = True, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200, return_sequences=True, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='selu'))\n",
    "model.add(Dense(50, activation='selu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "val_metrics = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 5\n",
    "\n",
    "@tf.function\n",
    "def calc_loss(x_train, y_train):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x_train, training=False)\n",
    "        loss_value = loss_function(y_train, logits)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "def batch_selector(data, idx):\n",
    "    largest_loss = 0\n",
    "    largest_loss_idx = idx\n",
    "\n",
    "    if idx < len(data) - length:\n",
    "        for i in range(idx, idx+length):\n",
    "            x_batch_train = data[i][0]\n",
    "            y_batch_train = data[i][1]\n",
    "            loss = calc_loss(x_batch_train, y_batch_train)\n",
    "            if loss > largest_loss:\n",
    "                largest_loss = loss\n",
    "                largest_loss_idx = i\n",
    "        return largest_loss_idx\n",
    "    else:\n",
    "        loss = calc_loss(data[idx][0], data[idx][1])\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/50\tLoss: 0.5006\tMetrics: 0.6912: \tValidation metrics: 0.7551: \t100% | 104/104 [00:03<00:00, 29.80it/s]\n",
      "Epoch 2/50\tLoss: 0.4800\tMetrics: 0.7474: \tValidation metrics: 0.7672: \t100% | 104/104 [00:01<00:00, 76.23it/s]\n",
      "Epoch 3/50\tLoss: 0.4787\tMetrics: 0.7579: \tValidation metrics: 0.7724: \t100% | 104/104 [00:01<00:00, 75.12it/s]\n",
      "Epoch 4/50\tLoss: 0.4771\tMetrics: 0.7620: \tValidation metrics: 0.7736: \t100% | 104/104 [00:01<00:00, 75.92it/s]\n",
      "Epoch 5/50\tLoss: 0.4678\tMetrics: 0.7670: \tValidation metrics: 0.7736: \t100% | 104/104 [00:01<00:00, 74.29it/s]\n",
      "Epoch 6/50\tLoss: 0.4667\tMetrics: 0.7682: \tValidation metrics: 0.7759: \t100% | 104/104 [00:01<00:00, 75.08it/s]\n",
      "Epoch 7/50\tLoss: 0.4625\tMetrics: 0.7704: \tValidation metrics: 0.7788: \t100% | 104/104 [00:01<00:00, 75.08it/s]\n",
      "Epoch 8/50\tLoss: 0.4577\tMetrics: 0.7728: \tValidation metrics: 0.7789: \t100% | 104/104 [00:01<00:00, 75.79it/s]\n",
      "Epoch 9/50\tLoss: 0.4553\tMetrics: 0.7744: \tValidation metrics: 0.7779: \t100% | 104/104 [00:01<00:00, 74.15it/s]\n",
      "Epoch 10/50\tLoss: 0.4628\tMetrics: 0.7758: \tValidation metrics: 0.7777: \t100% | 104/104 [00:01<00:00, 75.12it/s]\n",
      "Epoch 11/50\tLoss: 0.4497\tMetrics: 0.7765: \tValidation metrics: 0.7791: \t100% | 104/104 [00:01<00:00, 75.70it/s]\n",
      "Epoch 12/50\tLoss: 0.4553\tMetrics: 0.7785: \tValidation metrics: 0.7799: \t100% | 104/104 [00:01<00:00, 74.95it/s]\n",
      "Epoch 13/50\tLoss: 0.4499\tMetrics: 0.7783: \tValidation metrics: 0.7836: \t100% | 104/104 [00:01<00:00, 74.53it/s]\n",
      "Epoch 14/50\tLoss: 0.4543\tMetrics: 0.7789: \tValidation metrics: 0.7806: \t100% | 104/104 [00:01<00:00, 76.24it/s]\n",
      "Epoch 15/50\tLoss: 0.4458\tMetrics: 0.7784: \tValidation metrics: 0.7798: \t100% | 104/104 [00:01<00:00, 74.09it/s]\n",
      "Epoch 16/50\tLoss: 0.4428\tMetrics: 0.7794: \tValidation metrics: 0.7812: \t100% | 104/104 [00:01<00:00, 74.40it/s]\n",
      "Epoch 17/50\tLoss: 0.4463\tMetrics: 0.7820: \tValidation metrics: 0.7813: \t100% | 104/104 [00:01<00:00, 73.70it/s]\n",
      "Epoch 18/50\tLoss: 0.4458\tMetrics: 0.7819: \tValidation metrics: 0.7824: \t100% | 104/104 [00:01<00:00, 74.77it/s]\n",
      "Epoch 19/50\tLoss: 0.4522\tMetrics: 0.7823: \tValidation metrics: 0.7811: \t100% | 104/104 [00:01<00:00, 72.58it/s]\n",
      "Epoch 20/50\tLoss: 0.4502\tMetrics: 0.7821: \tValidation metrics: 0.7807: \t100% | 104/104 [00:01<00:00, 74.67it/s]\n",
      "Epoch 21/50\tLoss: 0.4497\tMetrics: 0.7843: \tValidation metrics: 0.7808: \t100% | 104/104 [00:01<00:00, 74.04it/s]\n",
      "Epoch 22/50\tLoss: 0.4414\tMetrics: 0.7816: \tValidation metrics: 0.7814: \t100% | 104/104 [00:01<00:00, 74.00it/s]\n",
      "Epoch 23/50\tLoss: 0.4362\tMetrics: 0.7841: \tValidation metrics: 0.7801: \t100% | 104/104 [00:01<00:00, 73.35it/s]\n",
      "Epoch 24/50\tLoss: 0.4370\tMetrics: 0.7838: \tValidation metrics: 0.7804: \t100% | 104/104 [00:01<00:00, 73.94it/s]\n",
      "Epoch 25/50\tLoss: 0.4423\tMetrics: 0.7851: \tValidation metrics: 0.7812: \t100% | 104/104 [00:01<00:00, 73.33it/s]\n",
      "Epoch 26/50\tLoss: 0.4414\tMetrics: 0.7837: \tValidation metrics: 0.7825: \t100% | 104/104 [00:01<00:00, 73.79it/s]\n",
      "Epoch 27/50\tLoss: 0.4357\tMetrics: 0.7863: \tValidation metrics: 0.7799: \t100% | 104/104 [00:01<00:00, 74.88it/s]\n",
      "Epoch 28/50\tLoss: 0.4329\tMetrics: 0.7856: \tValidation metrics: 0.7819: \t100% | 104/104 [00:01<00:00, 71.52it/s]\n",
      "Epoch 29/50\tLoss: 0.4471\tMetrics: 0.7860: \tValidation metrics: 0.7789: \t100% | 104/104 [00:01<00:00, 73.94it/s]\n",
      "Epoch 30/50\tLoss: 0.4392\tMetrics: 0.7875: \tValidation metrics: 0.7807: \t100% | 104/104 [00:01<00:00, 73.03it/s]\n",
      "Epoch 31/50\tLoss: 0.4339\tMetrics: 0.7857: \tValidation metrics: 0.7790: \t100% | 104/104 [00:01<00:00, 72.77it/s]\n",
      "Epoch 32/50\tLoss: 0.4424\tMetrics: 0.7865: \tValidation metrics: 0.7800: \t100% | 104/104 [00:01<00:00, 73.46it/s]\n",
      "Epoch 33/50\tLoss: 0.4302\tMetrics: 0.7871: \tValidation metrics: 0.7805: \t100% | 104/104 [00:01<00:00, 73.71it/s]\n",
      "Epoch 34/50\tLoss: 0.4280\tMetrics: 0.7878: \tValidation metrics: 0.7788: \t100% | 104/104 [00:01<00:00, 73.44it/s]\n",
      "Epoch 35/50\tLoss: 0.4379\tMetrics: 0.7879: \tValidation metrics: 0.7812: \t100% | 104/104 [00:01<00:00, 73.41it/s]\n",
      "Epoch 36/50\tLoss: 0.4341\tMetrics: 0.7878: \tValidation metrics: 0.7803: \t100% | 104/104 [00:01<00:00, 73.35it/s]\n",
      "Epoch 37/50\tLoss: 0.4245\tMetrics: 0.7878: \tValidation metrics: 0.7800: \t100% | 104/104 [00:01<00:00, 71.92it/s]\n",
      "Epoch 38/50\tLoss: 0.4310\tMetrics: 0.7889: \tValidation metrics: 0.7805: \t100% | 104/104 [00:01<00:00, 73.60it/s]\n",
      "Epoch 39/50\tLoss: 0.4323\tMetrics: 0.7888: \tValidation metrics: 0.7789: \t100% | 104/104 [00:01<00:00, 72.84it/s]\n",
      "Epoch 40/50\tLoss: 0.4268\tMetrics: 0.7894: \tValidation metrics: 0.7813: \t100% | 104/104 [00:01<00:00, 73.22it/s]\n",
      "Epoch 41/50\tLoss: 0.4269\tMetrics: 0.7892: \tValidation metrics: 0.7817: \t100% | 104/104 [00:01<00:00, 73.76it/s]\n",
      "Epoch 42/50\tLoss: 0.4323\tMetrics: 0.7891: \tValidation metrics: 0.7816: \t100% | 104/104 [00:01<00:00, 72.79it/s]\n",
      "Epoch 43/50\tLoss: 0.4246\tMetrics: 0.7884: \tValidation metrics: 0.7813: \t100% | 104/104 [00:01<00:00, 74.24it/s]\n",
      "Epoch 44/50\tLoss: 0.4196\tMetrics: 0.7898: \tValidation metrics: 0.7824: \t100% | 104/104 [00:01<00:00, 74.02it/s]\n",
      "Epoch 45/50\tLoss: 0.4260\tMetrics: 0.7896: \tValidation metrics: 0.7797: \t100% | 104/104 [00:01<00:00, 72.86it/s]\n",
      "Epoch 46/50\tLoss: 0.4312\tMetrics: 0.7891: \tValidation metrics: 0.7829: \t100% | 104/104 [00:01<00:00, 72.79it/s]\n",
      "Epoch 47/50\tLoss: 0.4249\tMetrics: 0.7895: \tValidation metrics: 0.7808: \t100% | 104/104 [00:01<00:00, 73.24it/s]\n",
      "Epoch 48/50\tLoss: 0.4333\tMetrics: 0.7899: \tValidation metrics: 0.7786: \t100% | 104/104 [00:01<00:00, 74.34it/s]\n",
      "Epoch 49/50\tLoss: 0.4257\tMetrics: 0.7890: \tValidation metrics: 0.7808: \t100% | 104/104 [00:01<00:00, 71.36it/s]\n",
      "Epoch 50/50\tLoss: 0.4347\tMetrics: 0.7914: \tValidation metrics: 0.7816: \t100% | 104/104 [00:01<00:00, 72.37it/s]\n"
     ]
    }
   ],
   "source": [
    "training = TrainingLoop(model, X_train, y_train, \n",
    "                        loss_function, \n",
    "                        optimizer, \n",
    "                        train_metrics, \n",
    "                        val_metrics, \n",
    "                        validation_split=0.1, \n",
    "                        batch_size=batch_size, \n",
    "                        batch_selection=batch_selector, \n",
    "                        length=length\n",
    "                        )\n",
    "training.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39/39 [==============================] - 0s 3ms/step - loss: 0.4634 - acc: 0.7835\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.4633648693561554, 0.7834864854812622]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  }
 ]
}