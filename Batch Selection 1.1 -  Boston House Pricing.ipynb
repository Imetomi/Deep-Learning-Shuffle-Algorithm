{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# needed to avoid a tf error\n",
    "try:\n",
    "    gpus= tf.config.experimental.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading models that we prepared earlier\n",
    "X_train = np.load(os.path.join('data', 'boston', 'boston_train_vectors.npy'), allow_pickle=True)\n",
    "X_test = np.load(os.path.join('data', 'boston', 'boston_test_vectors.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join('data', 'boston', 'boston_train_labels.npy'), allow_pickle=True)\n",
    "y_test = np.load(os.path.join('data', 'boston', 'boston_test_labels.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((404, 13), (404,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed to get reproducible results\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# building a small model as an experiment\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, activation='sigmoid'))\n",
    "    model.add(Dense(50, activation='sigmoid'))\n",
    "    model.add(Dense(50, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='sgd', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping is not used in the current implementation, but we plan to use it in the final model\n",
    "cb = [EarlyStopping(monitor=\"val_mae\", min_delta=0.01, patience=2, verbose=1, \n",
    "                    mode=\"auto\", baseline=None, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, batch_size=1, epochs=20, validation_split=0.1, callbacks=cb)"
   ]
  },
  {
   "source": [
    "# Using the custom training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our custom loop\n",
    "from loop import TrainingLoop\n",
    "# importing our batch selection algorithms\n",
    "from batch_selection import windowed_batch_selector, sorting_batch_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SGD oprimizer for training\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "# MSE loss function for this regression task\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "\n",
    "# using MAE as our secondary metric\n",
    "train_metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "val_metrics = tf.keras.metrics.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, batch_selector, epochs):\n",
    "    selectors = {windowed_batch_selector: 'windowed', sorting_batch_selector: 'sorting', None: 'original'}\n",
    "    print('\\n\\n'+selectors[batch_selector]+'\\n')\n",
    "    # defining the training class\n",
    "    training = TrainingLoop(model, X_train, y_train, \n",
    "        loss_function, \n",
    "        optimizer, \n",
    "        train_metrics, \n",
    "        val_metrics, \n",
    "        validation_split=0.1, \n",
    "        batch_size=batch_size,\n",
    "        batch_selection=batch_selector,\n",
    "        log_file = os.path.join('logs', selectors[batch_selector], 'boston_houses.csv')\n",
    "    )\n",
    "    # training the model\n",
    "    training.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    res = model.evaluate(X_test, y_test)\n",
    "    print(np.sqrt(res[0]), res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calc_loss(x_train, y_train, model, loss_function):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x_train, training=False)\n",
    "        loss_value = loss_function(y_train, logits)\n",
    "    return loss_value\n",
    "\n",
    "length = 5\n",
    "def windowed_batch_selector(data, idx, model, loss_function ):\n",
    "    largest_loss = 0\n",
    "    largest_loss_idx = idx\n",
    "\n",
    "    if idx < len(data) - length:\n",
    "        for i in range(idx, idx+length):\n",
    "            x_batch_train = data[i][0]\n",
    "            y_batch_train = data[i][1]\n",
    "            loss = calc_loss(x_batch_train, y_batch_train, model, loss_function)\n",
    "            if loss > largest_loss:\n",
    "                largest_loss = loss\n",
    "                largest_loss_idx = i\n",
    "        return largest_loss_idx\n",
    "    else:\n",
    "        loss = calc_loss(data[idx][0], data[idx][1], model, loss_function)\n",
    "        return idx\n",
    "\n",
    "\n",
    "losses = []\n",
    "def sorting_batch_selector(data, idx, model, loss_function):\n",
    "    global losses\n",
    "    if idx == 0:\n",
    "        for i in range(len(data)):\n",
    "            x_batch_train = data[i][0]\n",
    "            y_batch_train = data[i][1]\n",
    "            losses.append([i, float(calc_loss(x_batch_train, y_batch_train, model, loss_function))])\n",
    "        losses = sorted(losses, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "\n",
    "    return_idx = losses[idx][0]\n",
    "    if idx == len(data)-1:\n",
    "        losses.clear()\n",
    "    \n",
    "    return return_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\t  0% | 0/45 [00:00<?, ?it/s]\n",
      "\n",
      "sorting\n",
      "\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/20\tLoss: 38.369\tMetrics: 7.2289: \tValidation metrics: 6.7987: \t100% | 45/45 [00:00<00:00, 153.18it/s]\n",
      "Epoch 2/20\tLoss: 11.717\tMetrics: 6.2145: \tValidation metrics: 5.1483: \t100% | 45/45 [00:00<00:00, 376.50it/s]\n",
      "Epoch 3/20\tLoss: 8.8421\tMetrics: 4.7150: \tValidation metrics: 3.8345: \t100% | 45/45 [00:00<00:00, 346.62it/s]\n",
      "Epoch 4/20\tLoss: 2.0045\tMetrics: 3.7056: \tValidation metrics: 3.3208: \t100% | 45/45 [00:00<00:00, 353.63it/s]\n",
      "Epoch 5/20\tLoss: 2.7182\tMetrics: 3.6447: \tValidation metrics: 3.1772: \t100% | 45/45 [00:00<00:00, 309.90it/s]\n",
      "Epoch 6/20\tLoss: 1.9004\tMetrics: 3.5040: \tValidation metrics: 3.1053: \t100% | 45/45 [00:00<00:00, 283.02it/s]\n",
      "Epoch 7/20\tLoss: 1.9327\tMetrics: 3.3273: \tValidation metrics: 3.1047: \t100% | 45/45 [00:00<00:00, 340.13it/s]\n",
      "Epoch 8/20\tLoss: 2.2742\tMetrics: 3.2170: \tValidation metrics: 2.9917: \t100% | 45/45 [00:00<00:00, 320.26it/s]\n",
      "Epoch 9/20\tLoss: 2.3088\tMetrics: 3.1591: \tValidation metrics: 2.9477: \t100% | 45/45 [00:00<00:00, 352.42it/s]\n",
      "Epoch 10/20\tLoss: 2.4284\tMetrics: 3.1029: \tValidation metrics: 2.9339: \t100% | 45/45 [00:00<00:00, 335.68it/s]\n",
      "Epoch 11/20\tLoss: 2.4604\tMetrics: 3.0800: \tValidation metrics: 2.8931: \t100% | 45/45 [00:00<00:00, 309.91it/s]\n",
      "Epoch 12/20\tLoss: 2.4032\tMetrics: 2.9980: \tValidation metrics: 2.9071: \t100% | 45/45 [00:00<00:00, 340.12it/s]\n",
      "Epoch 13/20\tLoss: 2.0168\tMetrics: 2.9653: \tValidation metrics: 2.8837: \t100% | 45/45 [00:00<00:00, 337.85it/s]\n",
      "Epoch 14/20\tLoss: 2.0002\tMetrics: 2.7864: \tValidation metrics: 2.9367: \t100% | 45/45 [00:00<00:00, 379.83it/s]\n",
      "Epoch 15/20\tLoss: 2.2926\tMetrics: 2.7895: \tValidation metrics: 2.9722: \t100% | 45/45 [00:00<00:00, 323.90it/s]\n",
      "Epoch 16/20\tLoss: 1.5906\tMetrics: 2.6469: \tValidation metrics: 2.8922: \t100% | 45/45 [00:00<00:00, 310.47it/s]\n",
      "Epoch 17/20\tLoss: 2.1249\tMetrics: 2.7420: \tValidation metrics: 2.9457: \t100% | 45/45 [00:00<00:00, 323.42it/s]\n",
      "Epoch 18/20\tLoss: 2.0547\tMetrics: 2.7005: \tValidation metrics: 2.9133: \t100% | 45/45 [00:00<00:00, 335.95it/s]\n",
      "Epoch 19/20\tLoss: 2.4603\tMetrics: 2.5916: \tValidation metrics: 2.9210: \t100% | 45/45 [00:00<00:00, 337.07it/s]\n",
      "Epoch 20/20\tLoss: 2.7447\tMetrics: 2.6034: \tValidation metrics: 2.8586: \t100% | 45/45 [00:00<00:00, 337.63it/s]\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 14.2001 - mae: 2.7954\n",
      "3.768295417560028 2.795444965362549\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = build_model()\n",
    "train(model, X_train, y_train, batch_selector=sorting_batch_selector, epochs=epochs)\n",
    "evaluate(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}