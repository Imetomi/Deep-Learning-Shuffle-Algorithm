{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "Custom Training Loop copy.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-jAu6v10cCI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Both MNIST and Fashion-MNIST can be loaded from Keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "from loop import TrainingLoop"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdmoP7D_0cCK"
      },
      "source": [
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Convert image pixels to floats between 0 and 1\n",
        "X_train = x_train / 255\n",
        "X_test = x_test / 255\n",
        "\n",
        "# Convert output to one hot encoding\n",
        "Y_train = to_categorical(y_train, 10) \n",
        "Y_test = to_categorical(y_test, 10)\n",
        "\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frJ-kxzu5X0M"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTmYjsNH0cCK"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Input(shape = (28, 28, 1,)))\n",
        "model.add(layers.Conv2D(32, kernel_size = (3, 3), activation = \"relu\"))\n",
        "model.add(layers.MaxPooling2D( pool_size = (2, 2)))\n",
        "model.add(layers.Conv2D(64, kernel_size = (3, 3), activation = \"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation = \"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print( model.summary() )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1600)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                16010     \n=================================================================\nTotal params: 34,826\nTrainable params: 34,826\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOVLq9f10cCM"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "#validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "#validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RCVhcP0cCQ"
      },
      "source": [
        "train_data = list(train_dataset)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c19ykuh0cCQ"
      },
      "source": [
        "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRGEFBLB0cCQ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0vJTm1e0cCR"
      },
      "source": [
        "@tf.function\n",
        "def validation_step(x_val, y_val):\n",
        "    val_logits = model(x_val, training=False)\n",
        "    val_acc_metric.update_state(y_val, val_logits)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EcMxjCH90cCR"
      },
      "source": [
        "epochs = 0\n",
        "\n",
        "for epoch in range(epochs):    \n",
        "    steps = trange(len(train_data), bar_format=\"{desc}\\t{percentage:3.0f}% {r_bar}\")\n",
        "    for i in steps:\n",
        "        step = i\n",
        "        x_batch_train = train_data[i][0]\n",
        "        y_batch_train = train_data[i][1]\n",
        "        \n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        \n",
        "        steps.set_description(\"Epoch \" + str(epoch+1) + '/' + str(epochs) + \"\\tLoss: \" + str(float(loss_value))[:6]\n",
        "                              + \"\\tAccuracy: \" + str(float(train_acc_metric.result()))[:6])\n",
        "        \n",
        "        if i == len(train_data)-1:\n",
        "          \n",
        "            for x_batch_val, y_batch_val in validation_dataset:\n",
        "                validation_step(x_batch_val, y_batch_val)\n",
        "\n",
        "            steps.set_description(steps.desc + \"\\tValidation accuracy: \" + str(float(val_acc_metric.result()))[:6])\n",
        "\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "source": [
        "## Using our custom class to train the model."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "training = TrainingLoop(model, X_train, Y_train, loss_fn, optimizer, \n",
        "                        train_metrics=train_acc_metric, val_metrics=val_acc_metric, \n",
        "                        validation_split=0.1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20\tLoss: 1.6520\tMetrics: 0.7346: \tValidation metrics: 0.8267: \t100% | 843/843 [00:03<00:00, 245.44it/s]\n",
            "Epoch 2/20\tLoss: 1.6011\tMetrics: 0.8379: \tValidation metrics: 0.8477: \t100% | 843/843 [00:02<00:00, 389.15it/s]\n",
            "Epoch 3/20\tLoss: 1.5796\tMetrics: 0.8588: \tValidation metrics: 0.8627: \t100% | 843/843 [00:02<00:00, 383.51it/s]\n",
            "Epoch 4/20\tLoss: 1.5572\tMetrics: 0.8683: \tValidation metrics: 0.8718: \t100% | 843/843 [00:02<00:00, 395.86it/s]\n",
            "Epoch 5/20\tLoss: 1.6020\tMetrics: 0.8748: \tValidation metrics: 0.8635: \t100% | 843/843 [00:02<00:00, 400.14it/s]\n",
            "Epoch 6/20\tLoss: 1.5451\tMetrics: 0.8811: \tValidation metrics: 0.8771: \t100% | 843/843 [00:02<00:00, 395.51it/s]\n",
            "Epoch 7/20\tLoss: 1.5397\tMetrics: 0.8874: \tValidation metrics: 0.8797: \t100% | 843/843 [00:02<00:00, 395.48it/s]\n",
            "Epoch 8/20\tLoss: 1.5384\tMetrics: 0.8920: \tValidation metrics: 0.8818: \t100% | 843/843 [00:02<00:00, 398.06it/s]\n",
            "Epoch 9/20\tLoss: 1.5245\tMetrics: 0.8964: \tValidation metrics: 0.8862: \t100% | 843/843 [00:02<00:00, 400.85it/s]\n",
            "Epoch 10/20\tLoss: 1.5280\tMetrics: 0.9009: \tValidation metrics: 0.8907: \t100% | 843/843 [00:02<00:00, 392.73it/s]\n",
            "Epoch 11/20\tLoss: 1.5334\tMetrics: 0.9052: \tValidation metrics: 0.8924: \t100% | 843/843 [00:02<00:00, 394.90it/s]\n",
            "Epoch 12/20\tLoss: 1.5286\tMetrics: 0.9072: \tValidation metrics: 0.8949: \t100% | 843/843 [00:02<00:00, 388.23it/s]\n",
            "Epoch 13/20\tLoss: 1.5228\tMetrics: 0.9100: \tValidation metrics: 0.8954: \t100% | 843/843 [00:02<00:00, 392.11it/s]\n",
            "Epoch 14/20\tLoss: 1.5261\tMetrics: 0.9136: \tValidation metrics: 0.8954: \t100% | 843/843 [00:02<00:00, 387.72it/s]\n",
            "Epoch 15/20\tLoss: 1.5211\tMetrics: 0.9155: \tValidation metrics: 0.8953: \t100% | 843/843 [00:02<00:00, 394.05it/s]\n",
            "Epoch 16/20\tLoss: 1.5247\tMetrics: 0.9164: \tValidation metrics: 0.8980: \t100% | 843/843 [00:02<00:00, 395.62it/s]\n",
            "Epoch 17/20\tLoss: 1.5269\tMetrics: 0.9200: \tValidation metrics: 0.8968: \t100% | 843/843 [00:02<00:00, 393.53it/s]\n",
            "Epoch 18/20\tLoss: 1.5165\tMetrics: 0.9218: \tValidation metrics: 0.9012: \t100% | 843/843 [00:02<00:00, 369.20it/s]\n",
            "Epoch 19/20\tLoss: 1.5196\tMetrics: 0.9222: \tValidation metrics: 0.9015: \t100% | 843/843 [00:02<00:00, 376.30it/s]\n",
            "Epoch 20/20\tLoss: 1.5112\tMetrics: 0.9258: \tValidation metrics: 0.9007: \t100% | 843/843 [00:02<00:00, 378.14it/s]\n"
          ]
        }
      ],
      "source": [
        "training.train(20)"
      ]
    }
  ]
}