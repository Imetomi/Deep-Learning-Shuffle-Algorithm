{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "colab": {
      "name": "Custom Training Loop copy.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-jAu6v10cCI"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Both MNIST and Fashion-MNIST can be loaded from Keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "from loop import TrainingLoop"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdmoP7D_0cCK"
      },
      "source": [
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Convert image pixels to floats between 0 and 1\n",
        "X_train = x_train / 255\n",
        "X_test = x_test / 255\n",
        "\n",
        "# Convert output to one hot encoding\n",
        "Y_train = to_categorical(y_train, 10) \n",
        "Y_test = to_categorical(y_test, 10)\n",
        "\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frJ-kxzu5X0M"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTmYjsNH0cCK"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus= tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Input(shape = (28, 28, 1,)))\n",
        "model.add(layers.Conv2D(32, kernel_size = (3, 3), activation = \"relu\"))\n",
        "model.add(layers.MaxPooling2D( pool_size = (2, 2)))\n",
        "model.add(layers.Conv2D(64, kernel_size = (3, 3), activation = \"relu\"))\n",
        "model.add(layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation = \"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print( model.summary() )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1600)              0         \n_________________________________________________________________\ndense (Dense)                (None, 10)                16010     \n=================================================================\nTotal params: 34,826\nTrainable params: 34,826\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOVLq9f10cCM"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "#validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "#validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RCVhcP0cCQ"
      },
      "source": [
        "train_data = list(train_dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c19ykuh0cCQ"
      },
      "source": [
        "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRGEFBLB0cCQ"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0vJTm1e0cCR"
      },
      "source": [
        "@tf.function\n",
        "def validation_step(x_val, y_val):\n",
        "    val_logits = model(x_val, training=False)\n",
        "    val_acc_metric.update_state(y_val, val_logits)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EcMxjCH90cCR"
      },
      "source": [
        "epochs = 0\n",
        "\n",
        "for epoch in range(epochs):    \n",
        "    steps = trange(len(train_data), bar_format=\"{desc}\\t{percentage:3.0f}% {r_bar}\")\n",
        "    for i in steps:\n",
        "        step = i\n",
        "        x_batch_train = train_data[i][0]\n",
        "        y_batch_train = train_data[i][1]\n",
        "        \n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "        \n",
        "        steps.set_description(\"Epoch \" + str(epoch+1) + '/' + str(epochs) + \"\\tLoss: \" + str(float(loss_value))[:6]\n",
        "                              + \"\\tAccuracy: \" + str(float(train_acc_metric.result()))[:6])\n",
        "        \n",
        "        if i == len(train_data)-1:\n",
        "          \n",
        "            for x_batch_val, y_batch_val in validation_dataset:\n",
        "                validation_step(x_batch_val, y_batch_val)\n",
        "\n",
        "            steps.set_description(steps.desc + \"\\tValidation accuracy: \" + str(float(val_acc_metric.result()))[:6])\n",
        "\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "## Using our custom class to train the model."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "training = TrainingLoop(model, X_train, Y_train, loss_fn, optimizer, \n",
        "                        train_metrics=train_acc_metric, val_metrics=val_acc_metric, \n",
        "                        validation_split=0.1, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20\tLoss: 1.6529\tAccuracy: 0.7253: \tValidation accuracy: 0.7797: \t100% | 843/843 [00:03<00:00, 246.12it/s]\n",
            "Epoch 2/20\tLoss: 1.5955\tAccuracy: 0.8253: \tValidation accuracy: 0.8576: \t100% | 843/843 [00:02<00:00, 398.85it/s]\n",
            "Epoch 3/20\tLoss: 1.5948\tAccuracy: 0.8592: \tValidation accuracy: 0.8639: \t100% | 843/843 [00:02<00:00, 394.00it/s]\n",
            "Epoch 4/20\tLoss: 1.5843\tAccuracy: 0.8694: \tValidation accuracy: 0.8681: \t100% | 843/843 [00:02<00:00, 401.20it/s]\n",
            "Epoch 5/20\tLoss: 1.5571\tAccuracy: 0.8773: \tValidation accuracy: 0.8760: \t100% | 843/843 [00:02<00:00, 400.13it/s]\n",
            "Epoch 6/20\tLoss: 1.5678\tAccuracy: 0.8833: \tValidation accuracy: 0.8805: \t100% | 843/843 [00:02<00:00, 400.15it/s]\n",
            "Epoch 7/20\tLoss: 1.5528\tAccuracy: 0.8886: \tValidation accuracy: 0.8837: \t100% | 843/843 [00:02<00:00, 402.61it/s]\n",
            "Epoch 8/20\tLoss: 1.5447\tAccuracy: 0.8905: \tValidation accuracy: 0.8832: \t100% | 843/843 [00:02<00:00, 396.63it/s]\n",
            "Epoch 9/20\tLoss: 1.5446\tAccuracy: 0.8956: \tValidation accuracy: 0.8855: \t100% | 843/843 [00:02<00:00, 396.31it/s]\n",
            "Epoch 10/20\tLoss: 1.5551\tAccuracy: 0.8996: \tValidation accuracy: 0.8884: \t100% | 843/843 [00:02<00:00, 394.41it/s]\n",
            "Epoch 11/20\tLoss: 1.5299\tAccuracy: 0.9025: \tValidation accuracy: 0.8906: \t100% | 843/843 [00:02<00:00, 389.53it/s]\n",
            "Epoch 12/20\tLoss: 1.5382\tAccuracy: 0.9051: \tValidation accuracy: 0.8899: \t100% | 843/843 [00:02<00:00, 394.52it/s]\n",
            "Epoch 13/20\tLoss: 1.5321\tAccuracy: 0.9082: \tValidation accuracy: 0.8943: \t100% | 843/843 [00:02<00:00, 396.35it/s]\n",
            "Epoch 14/20\tLoss: 1.5408\tAccuracy: 0.9095: \tValidation accuracy: 0.8928: \t100% | 843/843 [00:02<00:00, 395.10it/s]\n",
            "Epoch 15/20\tLoss: 1.5293\tAccuracy: 0.9124: \tValidation accuracy: 0.8896: \t100% | 843/843 [00:02<00:00, 370.24it/s]\n",
            "Epoch 16/20\tLoss: 1.5352\tAccuracy: 0.9156: \tValidation accuracy: 0.8923: \t100% | 843/843 [00:02<00:00, 394.36it/s]\n",
            "Epoch 17/20\tLoss: 1.5146\tAccuracy: 0.9170: \tValidation accuracy: 0.8936: \t100% | 843/843 [00:02<00:00, 389.56it/s]\n",
            "Epoch 18/20\tLoss: 1.5085\tAccuracy: 0.9192: \tValidation accuracy: 0.8966: \t100% | 843/843 [00:02<00:00, 390.30it/s]\n",
            "Epoch 19/20\tLoss: 1.5265\tAccuracy: 0.9206: \tValidation accuracy: 0.8975: \t100% | 843/843 [00:02<00:00, 386.05it/s]\n",
            "Epoch 20/20\tLoss: 1.5208\tAccuracy: 0.9225: \tValidation accuracy: 0.8970: \t100% | 843/843 [00:02<00:00, 382.43it/s]\n"
          ]
        }
      ],
      "source": [
        "training.train(20)"
      ]
    }
  ]
}