{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data\n",
    "train_data = np.load(os.path.join(data_path, 'twitter_train_vectors.npy'), allow_pickle=True)\n",
    "test_data = np.load(os.path.join(data_path, 'twitter_test_vectors.npy'), allow_pickle=True)\n",
    "train_labels = np.load(os.path.join(data_path, 'twitter_train_labels.npy'), allow_pickle=True)\n",
    "test_labels = np.load(os.path.join(data_path, 'twitter_test_labels.npy'), allow_pickle=True)"
   ]
  },
  {
   "source": [
    "## Using word2vec as an input\n",
    "\n",
    "We have sentences in this dataset with different lengths, which means we can't feed these diractly into a neural network. As a solution I calculate the mean of the word vectors on a given window to get a fixed sentence length in every sample and drop the ones that do not meet the minimum sentence length criteria. The function below does exactly that.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_on_window(data, label, size):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "\n",
    "    for i in tqdm_n(range(len(data))):\n",
    "        sample = data[i]\n",
    "        if len(sample) >= size:\n",
    "            new_sample = []\n",
    "\n",
    "            # how many elements in a window\n",
    "            count = int(np.floor((len(sample) / size)))\n",
    "\n",
    "            # how many left\n",
    "            rest = int(len(sample) / size)\n",
    "            \n",
    "            # calculating mean in every window\n",
    "            for j in range(size-1):\n",
    "                new_sample.append(np.array(sample[j*count:(j+1)*count].mean(axis=0)))\n",
    "            if rest != 0:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count+rest].mean(axis=0)))\n",
    "            else:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count].mean(axis=0)))\n",
    "            new_data.append(np.array(new_sample))\n",
    "            new_labels.append(label[i])\n",
    "    return np.array(new_data), np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=119999.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a167ffc4bdbc4576bd5948f262cc1e99"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = average_on_window(train_data, train_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=40000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "567e119f6dae4ff1a77065df6292aafe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = average_on_window(test_data, test_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# defining random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# building an LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(5, return_sequences = True, activation='selu'))\n",
    "# using dropout as regularization\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200, return_sequences=True, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='selu'))\n",
    "model.add(Dense(50, activation='selu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=1024, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loop import TrainingLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# large batch size to test if our algorithm will be able to handle this\n",
    "batch_size = 1024\n",
    "epochs = 50\n",
    "\n",
    "log_path = 'logs/original/sentiment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining metrics\n",
    "train_metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "val_metrics = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the training loop object\n",
    "training = TrainingLoop(model, X_train, y_train, loss_function, optimizer, train_metrics, val_metrics, validation_split=0.1, batch_size=batch_size, log_file=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/50\tLoss: 0.5043\tMetrics: 0.6983: \tValidation metrics: 0.7473: \t100% | 104/104 [00:02<00:00, 41.39it/s]\n",
      "Epoch 2/50\tLoss: 0.4802\tMetrics: 0.7557: \tValidation metrics: 0.7632: \t100% | 104/104 [00:00<00:00, 155.87it/s]\n",
      "Epoch 3/50\tLoss: 0.4763\tMetrics: 0.7676: \tValidation metrics: 0.7715: \t100% | 104/104 [00:00<00:00, 158.01it/s]\n",
      "Epoch 4/50\tLoss: 0.4678\tMetrics: 0.7712: \tValidation metrics: 0.7763: \t100% | 104/104 [00:00<00:00, 154.47it/s]\n",
      "Epoch 5/50\tLoss: 0.4661\tMetrics: 0.7743: \tValidation metrics: 0.7783: \t100% | 104/104 [00:00<00:00, 158.19it/s]\n",
      "Epoch 6/50\tLoss: 0.4577\tMetrics: 0.7766: \tValidation metrics: 0.7800: \t100% | 104/104 [00:00<00:00, 157.13it/s]\n",
      "Epoch 7/50\tLoss: 0.4593\tMetrics: 0.7782: \tValidation metrics: 0.7836: \t100% | 104/104 [00:00<00:00, 154.64it/s]\n",
      "Epoch 8/50\tLoss: 0.4565\tMetrics: 0.7796: \tValidation metrics: 0.7820: \t100% | 104/104 [00:00<00:00, 158.28it/s]\n",
      "Epoch 9/50\tLoss: 0.4499\tMetrics: 0.7807: \tValidation metrics: 0.7825: \t100% | 104/104 [00:00<00:00, 157.25it/s]\n",
      "Epoch 10/50\tLoss: 0.4529\tMetrics: 0.7821: \tValidation metrics: 0.7819: \t100% | 104/104 [00:00<00:00, 157.30it/s]\n",
      "Epoch 11/50\tLoss: 0.4431\tMetrics: 0.7841: \tValidation metrics: 0.7807: \t100% | 104/104 [00:00<00:00, 159.95it/s]\n",
      "Epoch 12/50\tLoss: 0.4535\tMetrics: 0.7838: \tValidation metrics: 0.7800: \t100% | 104/104 [00:00<00:00, 156.92it/s]\n",
      "Epoch 13/50\tLoss: 0.4417\tMetrics: 0.7853: \tValidation metrics: 0.7816: \t100% | 104/104 [00:00<00:00, 158.01it/s]\n",
      "Epoch 14/50\tLoss: 0.4455\tMetrics: 0.7850: \tValidation metrics: 0.7816: \t100% | 104/104 [00:00<00:00, 154.35it/s]\n",
      "Epoch 15/50\tLoss: 0.4368\tMetrics: 0.7851: \tValidation metrics: 0.7834: \t100% | 104/104 [00:00<00:00, 159.12it/s]\n",
      "Epoch 16/50\tLoss: 0.4464\tMetrics: 0.7860: \tValidation metrics: 0.7816: \t100% | 104/104 [00:00<00:00, 157.34it/s]\n",
      "Epoch 17/50\tLoss: 0.4367\tMetrics: 0.7855: \tValidation metrics: 0.7817: \t100% | 104/104 [00:00<00:00, 151.93it/s]\n",
      "Epoch 18/50\tLoss: 0.4350\tMetrics: 0.7864: \tValidation metrics: 0.7799: \t100% | 104/104 [00:00<00:00, 158.27it/s]\n",
      "Epoch 19/50\tLoss: 0.4482\tMetrics: 0.7871: \tValidation metrics: 0.7824: \t100% | 104/104 [00:00<00:00, 158.32it/s]\n",
      "Epoch 20/50\tLoss: 0.4459\tMetrics: 0.7874: \tValidation metrics: 0.7824: \t100% | 104/104 [00:00<00:00, 154.66it/s]\n",
      "Epoch 21/50\tLoss: 0.4308\tMetrics: 0.7875: \tValidation metrics: 0.7829: \t100% | 104/104 [00:00<00:00, 159.33it/s]\n",
      "Epoch 22/50\tLoss: 0.4341\tMetrics: 0.7878: \tValidation metrics: 0.7811: \t100% | 104/104 [00:00<00:00, 155.48it/s]\n",
      "Epoch 23/50\tLoss: 0.4328\tMetrics: 0.7887: \tValidation metrics: 0.7827: \t100% | 104/104 [00:00<00:00, 154.03it/s]\n",
      "Epoch 24/50\tLoss: 0.4385\tMetrics: 0.7893: \tValidation metrics: 0.7827: \t100% | 104/104 [00:00<00:00, 157.43it/s]\n",
      "Epoch 25/50\tLoss: 0.4408\tMetrics: 0.7887: \tValidation metrics: 0.7827: \t100% | 104/104 [00:00<00:00, 158.70it/s]\n",
      "Epoch 26/50\tLoss: 0.4309\tMetrics: 0.7890: \tValidation metrics: 0.7826: \t100% | 104/104 [00:00<00:00, 150.83it/s]\n",
      "Epoch 27/50\tLoss: 0.4308\tMetrics: 0.7899: \tValidation metrics: 0.7817: \t100% | 104/104 [00:00<00:00, 154.79it/s]\n",
      "Epoch 28/50\tLoss: 0.4308\tMetrics: 0.7897: \tValidation metrics: 0.7823: \t100% | 104/104 [00:00<00:00, 152.68it/s]\n",
      "Epoch 29/50\tLoss: 0.4334\tMetrics: 0.7900: \tValidation metrics: 0.7831: \t100% | 104/104 [00:00<00:00, 153.96it/s]\n",
      "Epoch 30/50\tLoss: 0.4296\tMetrics: 0.7909: \tValidation metrics: 0.7825: \t100% | 104/104 [00:00<00:00, 152.32it/s]\n",
      "Epoch 31/50\tLoss: 0.4348\tMetrics: 0.7908: \tValidation metrics: 0.7821: \t100% | 104/104 [00:00<00:00, 153.10it/s]\n",
      "Epoch 32/50\tLoss: 0.4329\tMetrics: 0.7908: \tValidation metrics: 0.7820: \t100% | 104/104 [00:00<00:00, 153.62it/s]\n",
      "Epoch 33/50\tLoss: 0.4259\tMetrics: 0.7912: \tValidation metrics: 0.7824: \t100% | 104/104 [00:00<00:00, 154.41it/s]\n",
      "Epoch 34/50\tLoss: 0.4251\tMetrics: 0.7913: \tValidation metrics: 0.7824: \t100% | 104/104 [00:00<00:00, 156.94it/s]\n",
      "Epoch 35/50\tLoss: 0.4345\tMetrics: 0.7904: \tValidation metrics: 0.7820: \t100% | 104/104 [00:00<00:00, 153.51it/s]\n",
      "Epoch 36/50\tLoss: 0.4271\tMetrics: 0.7911: \tValidation metrics: 0.7832: \t100% | 104/104 [00:00<00:00, 153.31it/s]\n",
      "Epoch 37/50\tLoss: 0.4222\tMetrics: 0.7911: \tValidation metrics: 0.7840: \t100% | 104/104 [00:00<00:00, 152.87it/s]\n",
      "Epoch 38/50\tLoss: 0.4243\tMetrics: 0.7920: \tValidation metrics: 0.7845: \t100% | 104/104 [00:00<00:00, 158.01it/s]\n",
      "Epoch 39/50\tLoss: 0.4312\tMetrics: 0.7929: \tValidation metrics: 0.7827: \t100% | 104/104 [00:00<00:00, 155.47it/s]\n",
      "Epoch 40/50\tLoss: 0.4307\tMetrics: 0.7925: \tValidation metrics: 0.7836: \t100% | 104/104 [00:00<00:00, 152.21it/s]\n",
      "Epoch 41/50\tLoss: 0.4205\tMetrics: 0.7931: \tValidation metrics: 0.7850: \t100% | 104/104 [00:00<00:00, 151.58it/s]\n",
      "Epoch 42/50\tLoss: 0.4208\tMetrics: 0.7933: \tValidation metrics: 0.7850: \t100% | 104/104 [00:00<00:00, 153.82it/s]\n",
      "Epoch 43/50\tLoss: 0.4292\tMetrics: 0.7935: \tValidation metrics: 0.7849: \t100% | 104/104 [00:00<00:00, 155.33it/s]\n",
      "Epoch 44/50\tLoss: 0.4185\tMetrics: 0.7922: \tValidation metrics: 0.7838: \t100% | 104/104 [00:00<00:00, 152.98it/s]\n",
      "Epoch 45/50\tLoss: 0.4238\tMetrics: 0.7932: \tValidation metrics: 0.7847: \t100% | 104/104 [00:00<00:00, 152.61it/s]\n",
      "Epoch 46/50\tLoss: 0.4208\tMetrics: 0.7940: \tValidation metrics: 0.7847: \t100% | 104/104 [00:00<00:00, 155.58it/s]\n",
      "Epoch 47/50\tLoss: 0.4183\tMetrics: 0.7949: \tValidation metrics: 0.7851: \t100% | 104/104 [00:00<00:00, 156.12it/s]\n",
      "Epoch 48/50\tLoss: 0.4251\tMetrics: 0.7930: \tValidation metrics: 0.7853: \t100% | 104/104 [00:00<00:00, 147.32it/s]\n",
      "Epoch 49/50\tLoss: 0.4172\tMetrics: 0.7930: \tValidation metrics: 0.7841: \t100% | 104/104 [00:00<00:00, 154.76it/s]\n",
      "Epoch 50/50\tLoss: 0.4266\tMetrics: 0.7948: \tValidation metrics: 0.7846: \t100% | 104/104 [00:00<00:00, 147.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "training.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39/39 [==============================] - 0s 2ms/step - loss: 0.4575 - acc: 0.7875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.45750078558921814, 0.7874637842178345]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# quick evaluation\n",
    "model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually they get an accuracy of about 83% on this dataset. I am using only a tenth of the original size and I achieved 78% with an LSTM network. This could be better with some more tuning but in this project we're focusing on our batch selection algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}