{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(data_path, 'twitter_train_vectors.npy'), allow_pickle=True)\n",
    "test_data = np.load(os.path.join(data_path, 'twitter_test_vectors.npy'), allow_pickle=True)\n",
    "train_labels = np.load(os.path.join(data_path, 'twitter_train_labels.npy'), allow_pickle=True)\n",
    "test_labels = np.load(os.path.join(data_path, 'twitter_test_labels.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_on_window(data, label, size):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    for i in tqdm_n(range(len(data))):\n",
    "        sample = data[i]\n",
    "        if len(sample) >= size:\n",
    "            new_sample = []\n",
    "            count = int(np.floor((len(sample) / size)))\n",
    "            rest = int(len(sample) / size)\n",
    "            for j in range(size-1):\n",
    "                new_sample.append(np.array(sample[j*count:(j+1)*count].mean(axis=0)))\n",
    "            if rest != 0:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count+rest].mean(axis=0)))\n",
    "            else:\n",
    "                new_sample.append(np.array(sample[(size-1)*count:(size)*count].mean(axis=0)))\n",
    "            new_data.append(np.array(new_sample))\n",
    "            new_labels.append(label[i])\n",
    "    return np.array(new_data), np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cef7bfdece9464187fab57a829286f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = average_on_window(train_data, train_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4565a050bec478ebc7674b56870072c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = average_on_window(test_data, test_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(5, return_sequences = True, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200, return_sequences=True, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False, activation='selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='selu'))\n",
    "model.add(Dense(50, activation='selu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=0.001, patience=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107277 samples, validate on 11920 samples\n",
      "Epoch 1/50\n",
      "107277/107277 [==============================] - 2s 23us/step - loss: 0.5676 - acc: 0.6996 - val_loss: 0.5002 - val_acc: 0.7551\n",
      "Epoch 2/50\n",
      "107277/107277 [==============================] - 1s 11us/step - loss: 0.5010 - acc: 0.7558 - val_loss: 0.4832 - val_acc: 0.7628\n",
      "Epoch 3/50\n",
      "107277/107277 [==============================] - 1s 11us/step - loss: 0.4877 - acc: 0.7663 - val_loss: 0.4742 - val_acc: 0.7718\n",
      "Epoch 4/50\n",
      "107277/107277 [==============================] - 1s 11us/step - loss: 0.4789 - acc: 0.7708 - val_loss: 0.4678 - val_acc: 0.7764\n",
      "Epoch 5/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4735 - acc: 0.7743 - val_loss: 0.4660 - val_acc: 0.7788\n",
      "Epoch 6/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4705 - acc: 0.7762 - val_loss: 0.4659 - val_acc: 0.7782\n",
      "Epoch 7/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4673 - acc: 0.7788 - val_loss: 0.4638 - val_acc: 0.7790\n",
      "Epoch 8/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4652 - acc: 0.7794 - val_loss: 0.4624 - val_acc: 0.7783\n",
      "Epoch 9/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4627 - acc: 0.7817 - val_loss: 0.4610 - val_acc: 0.7794\n",
      "Epoch 10/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4616 - acc: 0.7813 - val_loss: 0.4590 - val_acc: 0.7830\n",
      "Epoch 11/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4609 - acc: 0.7833 - val_loss: 0.4625 - val_acc: 0.7805\n",
      "Epoch 12/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4584 - acc: 0.7841 - val_loss: 0.4582 - val_acc: 0.7822\n",
      "Epoch 13/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4579 - acc: 0.7851 - val_loss: 0.4573 - val_acc: 0.7844\n",
      "Epoch 14/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4559 - acc: 0.7852 - val_loss: 0.4572 - val_acc: 0.7826\n",
      "Epoch 15/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4562 - acc: 0.7849 - val_loss: 0.4576 - val_acc: 0.7814\n",
      "Epoch 16/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4549 - acc: 0.7868 - val_loss: 0.4594 - val_acc: 0.7802\n",
      "Epoch 17/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4542 - acc: 0.7853 - val_loss: 0.4587 - val_acc: 0.7826\n",
      "Epoch 18/50\n",
      "107277/107277 [==============================] - 1s 12us/step - loss: 0.4533 - acc: 0.7869 - val_loss: 0.4576 - val_acc: 0.7821\n",
      "CPU times: user 46.7 s, sys: 4.21 s, total: 50.9 s\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f0e7464a9d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=1024, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39725/39725 [==============================] - 3s 88us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4573823452235919, 0.7847954630851746]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually they get an accuracy of about 83% on this dataset. I am using only a tenth of the original size and I achieved 78% with an LSTM network. This could be better with some more tuning but in this project we're focusing on our batch selection algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
